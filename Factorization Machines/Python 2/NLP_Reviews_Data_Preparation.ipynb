{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Prep on Datasets\n",
    "\n",
    "This notebook contains code to:\n",
    "- Execute Text Mining / NLP on reviews\n",
    "- Parse item and rating datasets\n",
    "\n",
    "This can be used to run models such as:\n",
    "- Factorization Machines\n",
    "- Content Based Filtering Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "rating = getDF('reviews_Digital_Music.json.gz')\n",
    "meta = getDF('meta_Digital_Music.json.gz')\n",
    "\n",
    "print(time.time() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nmetadata dimensions\")\n",
    "print(meta.shape)\n",
    "print(\"\\nrating dimensions\")\n",
    "print(rating.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ratings = rating\n",
    "df_items = meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ratings.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_filter = pd.DataFrame(df_ratings['asin'])\n",
    "item_filter = item_filter.groupby('asin').size()\n",
    "item_filter = item_filter.to_frame().reset_index()\n",
    "item_filter.columns.values[1] = 'count'\n",
    "item_filter = item_filter.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_filter = item_filter.tail(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(item_filter['count'].mean())\n",
    "print(item_filter['count'].max())\n",
    "print(item_filter['count'].min())\n",
    "print(len(item_filter))\n",
    "item_filter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_filter = item_filter[['asin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ratings = pd.merge(left=df_ratings,right=item_filter, left_on='asin', right_on='asin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Text Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing StopWords through NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get usual stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "\n",
    "\n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    # to improve execution time this conversion should be done once\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "     \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings['cleanedreviews'] = df_ratings['reviewText'].apply(review_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from pandas import DataFrame \n",
    "reviews = df_ratings[\"cleanedreviews\"] \n",
    "countVector = CountVectorizer(max_features = 500, stop_words='english') \n",
    "transformedReviews = countVector.fit_transform(reviews) \n",
    "\n",
    "dfReviews = DataFrame(transformedReviews.A, columns=countVector.get_feature_names())\n",
    "dfReviews = dfReviews.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfReviews[dfReviews>0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the review text to the ratings dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ratings2 = pd.merge(df_ratings, dfReviews, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ratings2.shape)\n",
    "df_ratings2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning out the dataframe, selecting only important columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_ratings = df_ratings.drop(['reviewerName', \"helpful\", \"reviewText\", \"summary\", \"reviewTime\", \"cleanedreviews\"], 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_ratings['overall_x'] = df_ratings['overall_x'].astype(int)\n",
    "print(df_ratings.shape)\n",
    "df_ratings.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ratings2.to_csv(\"NLP_ratings_all_top1000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treating the MetaData Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.head(3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = df_items\n",
    "a['categories'] = a['categories'].astype(str)\n",
    "a['cleanedreviews'] = a['categories'].apply(review_to_words)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_items2.to_csv(\"metadata_NLP.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cats = a[\"cleanedreviews\"] \n",
    "countVector = CountVectorizer(max_features = 100, stop_words='english') \n",
    "transformedCategories = countVector.fit_transform(cats) \n",
    "\n",
    "dfCategories = DataFrame(transformedCategories.A, columns=countVector.get_feature_names())\n",
    "df_items2 = pd.merge(df_items, dfCategories, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the price and selecting relevant columns - price and ids"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# normalizing price\n",
    "df_items[\"price\"] = df_items[\"price\"].astype(float)\n",
    "\n",
    "mean_price = df_items.price.dropna().mean()\n",
    "max_price = df_items.price.dropna().max()\n",
    "min_price = df_items.price.dropna().min()\n",
    "\n",
    "\n",
    "df_items[\"price\"] = df_items[\"price\"].apply(lambda x: (x - min_price ) / (max_price -min_price ))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_items.to_csv(\"NLP_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
